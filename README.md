# Чеклист разработчки патча

**1. Сбор требований и постановка задачи**

1.1. Создание целевых сущностей.
Запросить у аналитиков ЛМку. Выяснить:
1.1.1 Какие поля должны быть ключевыми? Есть ли в этих полях пропуски на источнике? (такое бывает) Если есть, то чем их заполнять?
1.1.2. По каким полям следует делать партицирование?
1.1.3. Как правильно маппить типы данных? Точно ли маппинг полей в ЛМке корректный?
1.1.4. Нужно ли забирать с источника комментарии к полям и сущностям?
1.1.5. Нужно ли создавать в целевой системе БД, или она считается уже созданной? (по умолчанию - нужно)
1.1.6. Какие у таблиц должны быть метаполя и как их правильно заполнять? Подробнее про этом см. далее

1.2. Создание загрузок
Описать тип загрузки в целевую систему. Выяснить тип загрузки - потоковая или пакетная.

Пакетная загрузка делается с помощью DAG'ов в Airflow, управляемых из УМа. DAG'и разрабатываются в соответствии с [шаблоном](https://confluence.moscow.alfaintra.net/pages/viewpage.action?pageId=1780104534) из документации ЕИСа. 
Выяснить
1.2.1. Какие из 6 тасок (типов загрузки) DAG'а следует реализовать?
1.2.2. В каком режиме будет работать DAG? Если по расписанию, то по какому?
1.2.3. Какие параметры из УМа следует передавать в таски? Что следует вынести в УМ, что оставить в самих тасках к ДАГу? Основная идея - параметризовать все с помощью УМа настолько, насколько это возможно.
1.2.4. Нужно ли реализовать логирование в УМ? Какие параметры следует логировать? Как это правильно сделать с т.з. внутренней организации УМа? Логирование в УМе работает только с регламентами, но не с атомарными потоками. Причем только с регламентами, поставленным на расписание. Поэтому если нужно логироваться, то в любом случае придется делать регламент в УМе и ставить его на расписание.
1.2.5. Какие метаполя следует добавить в целевые сущности для хранения информации, связанной с загрузкой данных? По умолчанию берем полный список из 12 метаполей из [стандарта](https://confluence.moscow.alfaintra.net/pages/viewpage.action?pageId=1757988205) архитектуры ЕИС. Чем заполнять эти поля при разных типах загрузки? Как эти поля будут обновляться с течением времени?

**2. Разработка**

Общие рекомендации
1. Сразу именовать и раскладывать все по директориям в соответствии с [документацией](https://confluence.moscow.alfaintra.net/display/BDP/ICEBERG) ЕИСа. (!!!) Сэкономит много времени на этапе сдачи патча.
2. В качестве референса следует использовать наш патч,  который одобрили и успешно выкатили на прод. Он лежит в ветке `CIDS-3` в рабочем [репозитории](https://git.moscow.alfaintra.net/projects/BI_CIDS/repos/cids/browse?at=refs%2Fheads%2FCIDS-3) на битбакете.

2.1. Создание целевых сущностей.
2.1.1. Где брать информацию? - При написании ddl сверяться с [документацией](https://confluence.moscow.alfaintra.net/display/BDP/ICEBERG). Там, в частности, описано:
* Как правильно называть сущности и БД для них
* Как правильно располагать сущности в бакете s3
* Какие у таблиц должны быть свойства `tblproperties`
* Как настроить в таблицах ключи и партицирование
2.1.2. Как должен выглядеть DDL для таблиц? - Последовательность команд: `DROP TABLE IF EXISTS <table_name>` -> `CREATE TABLE IF NOT EXISTS <table_name>` -> `ALTER TABLE <table_name> SET IDENTIFIER FIELDS <identifier_field_1>, <identifier_field_2>;` -> `ALTER TABLE <table_name> WRITE DISTRIBUTED BY PARTITION LOCALLY ORDERED BY <partition_field_1> ASC NULLS LAST, <partition_field_2> ASC NULLS LAST;`
2.1.3. Как должен выглядеть DDL для БД? - `DROP DATABASE IF EXISTS <db_name>` -> `CREATE DATABASE IF NOT EXISTS <db_name>`
2.1.4. Какие у таблицы должны быть свойства? - Полный сбор статистик (`write.metadata.metrics.column.<column_name> = 'full'`) следует указывать для всех ключевых атрибутов и атрибутов, по которым выполянется партицирование таблицы. Другие свойства можно указывать без изменений:
```
'write.parquet.compression-codec' = 'zstd',
'write.parquet.compression-level' = '3',
'history.expire.max-snapshot-age-ms' = '86400000'
'format' = 'iceberg/parquet',
'format-version' = '2',
'write.delete.mode' = 'merge-on-read',
'write.merge.mode' = 'merge-on-read',
'write.update.mode' = 'merge-on-read',
'write.distribution-mode' = 'hash',
'write.metadata.delete-after-commit.enabled' = 'true',
'write.metadata.previous-versions-max' = '100',
```
2.1.5. Платформа для сборки патча не умеет работать с кодировками UTF. Это незаметно до тех пор, пока у тебя исходники не содержат не-ASCII символов. В противном же случае, - например, если у тебя в файлах есть комментарии к атрибутам или к коду на русском языке, - при работе с этими файлами следует всегда (!) явно (!) переключать в редакторе кода кодировку с UTF-8 на Windows-1251. Иначе можно легко по невнимательности сохранить файл в неправильной кодировке и испортить его, придется откатываться до последнего коммита в гите и начинать всю работу заново. **Внимательнее с кодировками!**
2.1.6. DDL-скрипты готовы. Как правильно их назвать и сложить в патч?
Имя ddl для таблицы: `<source_system>__<database_name>__<schema_name>__<table_name>.sql`
Здесь:
* `<source_system>` - наименование системы-источника
* `<database_name>` - имя БД в источнике
* `<schema_name>` - имя схемы в БД на источнике
* `<table_name>` - имя таблицы в БД на источнике
Пример: пусть у нас есть система-источник `bix`, которая хранит в БД `bixdb` схему `public`, в которой лежит таблица `claimitems`. Тогда скрипт будет называться `bix__bixdb__public__claimitems.sql`.
Путь до ddl для таблиц: `cids\DB\Spark\CIDS\ICEBERG\table\<table_ddl_name>.sql`.
Имя ddl для БД: `<source_system>__<database_name>.sql`
Путь до ddl для БД: `cids\DB\Spark\CIDS\ICEBERG\tablespace\<db_ddl_name>.sql`. Используется папка tablespace, т.к. папка database пока не поддерживается девопс-конвеером и не проходит nft-тесты.
Для нашего примера DDL-скрипт для БД будет называться `bix__bixdb.sql`

2.2 Создание потоков загрузки и их интеграция с УМом
