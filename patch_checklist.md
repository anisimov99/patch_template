---


---

<h1 id="чеклист-разработки-патча">Чеклист разработки патча</h1>
<p><strong>1. Сбор требований и постановка задачи</strong></p>
<p>1.1. Создание целевых сущностей.<br>
Запросить у аналитиков ЛМку. Выяснить:<br>
1.1.1 Какие поля должны быть ключевыми? Есть ли в этих полях пропуски на источнике? (такое бывает) Если есть, то чем их заполнять?<br>
1.1.2. По каким полям следует делать партицирование?<br>
1.1.3. Как правильно маппить типы данных? Точно ли маппинг полей в ЛМке корректный?<br>
1.1.4. Нужно ли забирать с источника комментарии к полям и сущностям?<br>
1.1.5. Нужно ли создавать в целевой системе БД, или она считается уже созданной? (по умолчанию - нужно)<br>
1.1.6. Какие у таблиц должны быть метаполя и как их правильно заполнять? Подробнее про этом см. далее</p>
<p>1.2. Создание загрузок<br>
Описать тип загрузки в целевую систему. Выяснить тип загрузки - потоковая или пакетная.</p>
<p>Пакетная загрузка делается с помощью DAG’ов в Airflow, управляемых из УМа. DAG’и разрабатываются в соответствии с <a href="https://confluence.moscow.alfaintra.net/pages/viewpage.action?pageId=1780104534">шаблоном</a> из документации ЕИСа.<br>
Выяснить<br>
1.2.1. Какие из 6 тасок (типов загрузки) DAG’а следует реализовать?<br>
1.2.2. В каком режиме будет работать DAG? Если по расписанию, то по какому?<br>
1.2.3. Какие параметры из УМа следует передавать в таски? Что следует вынести в УМ, что оставить в самих тасках к ДАГу? Основная идея - параметризовать все с помощью УМа настолько, насколько это возможно.<br>
1.2.4. Нужно ли реализовать логирование в УМ? Какие параметры следует логировать? Как это правильно сделать с т.з. внутренней организации УМа? Логирование в УМе работает только с регламентами, но не с атомарными потоками. Причем только с регламентами, поставленным на расписание. Поэтому если нужно логироваться, то в любом случае придется делать регламент в УМе и ставить его на расписание.<br>
1.2.5. Какие метаполя следует добавить в целевые сущности для хранения информации, связанной с загрузкой данных? По умолчанию берем полный список из 12 метаполей из <a href="https://confluence.moscow.alfaintra.net/pages/viewpage.action?pageId=1757988205">стандарта</a> архитектуры ЕИС. Чем заполнять эти поля при разных типах загрузки? Как эти поля будут обновляться с течением времени?</p>
<p><strong>2. Разработка</strong></p>
<p>Общие рекомендации</p>
<ol>
<li>Сразу именовать и раскладывать все по директориям в соответствии с <a href="https://confluence.moscow.alfaintra.net/display/BDP/ICEBERG">документацией</a> ЕИСа. (!!!) Сэкономит много времени на этапе сдачи патча.</li>
<li>В качестве референса следует использовать наш патч,  который одобрили и успешно выкатили на прод. Он лежит в ветке <code>CIDS-3</code> в рабочем <a href="https://git.moscow.alfaintra.net/projects/BI_CIDS/repos/cids/browse?at=refs%2Fheads%2FCIDS-3">репозитории</a> на битбакете.</li>
</ol>
<p>2.1. Создание целевых сущностей.<br>
2.1.1. Где брать информацию? - При написании ddl сверяться с <a href="https://confluence.moscow.alfaintra.net/display/BDP/ICEBERG">документацией</a>. Там, в частности, описано:</p>
<ul>
<li>Как правильно называть сущности и БД для них</li>
<li>Как правильно располагать сущности в бакете s3</li>
<li>Какие у таблиц должны быть свойства <code>tblproperties</code></li>
<li>Как настроить в таблицах ключи и партицирование<br>
2.1.2. Как должен выглядеть DDL для таблиц? - Последовательность команд: <code>DROP TABLE IF EXISTS &lt;table_name&gt;</code> -&gt; <code>CREATE TABLE IF NOT EXISTS &lt;table_name&gt;</code> -&gt; <code>ALTER TABLE &lt;table_name&gt; SET IDENTIFIER FIELDS &lt;identifier_field_1&gt;, &lt;identifier_field_2&gt;;</code> -&gt; <code>ALTER TABLE &lt;table_name&gt; WRITE DISTRIBUTED BY PARTITION LOCALLY ORDERED BY &lt;partition_field_1&gt; ASC NULLS LAST, &lt;partition_field_2&gt; ASC NULLS LAST;</code><br>
2.1.3. Как должен выглядеть DDL для БД? - <code>DROP DATABASE IF EXISTS &lt;db_name&gt;</code> -&gt; <code>CREATE DATABASE IF NOT EXISTS &lt;db_name&gt;</code><br>
2.1.4. Какие у таблицы должны быть свойства? - Полный сбор статистик (<code>write.metadata.metrics.column.&lt;column_name&gt; = 'full'</code>) следует указывать для всех ключевых атрибутов и атрибутов, по которым выполянется партицирование таблицы. Другие свойства можно указывать без изменений:</li>
</ul>
<pre><code>'write.parquet.compression-codec' = 'zstd',
'write.parquet.compression-level' = '3',
'history.expire.max-snapshot-age-ms' = '86400000'
'format' = 'iceberg/parquet',
'format-version' = '2',
'write.delete.mode' = 'merge-on-read',
'write.merge.mode' = 'merge-on-read',
'write.update.mode' = 'merge-on-read',
'write.distribution-mode' = 'hash',
'write.metadata.delete-after-commit.enabled' = 'true',
'write.metadata.previous-versions-max' = '100',
</code></pre>
<p>2.1.5. Платформа для сборки патча не умеет работать с кодировками UTF. Это незаметно до тех пор, пока у тебя исходники не содержат не-ASCII символов. В противном же случае, - например, если у тебя в файлах есть комментарии к атрибутам или к коду на русском языке, - при работе с этими файлами следует всегда (!) явно (!) переключать в редакторе кода кодировку с UTF-8 на Windows-1251. Иначе можно легко по невнимательности сохранить файл в неправильной кодировке и испортить его, придется откатываться до последнего коммита в гите и начинать всю работу заново. <strong>Внимательнее с кодировками!</strong><br>
2.1.6. DDL-скрипты готовы. Как правильно их назвать и сложить в патч?<br>
Имя ddl для таблицы: <code>&lt;source_system&gt;__&lt;database_name&gt;__&lt;schema_name&gt;__&lt;table_name&gt;.sql</code><br>
Здесь:</p>
<ul>
<li><code>&lt;source_system&gt;</code> - наименование системы-источника</li>
<li><code>&lt;database_name&gt;</code> - имя БД в источнике</li>
<li><code>&lt;schema_name&gt;</code> - имя схемы в БД на источнике</li>
<li><code>&lt;table_name&gt;</code> - имя таблицы в БД на источнике<br>
Пример: пусть у нас есть система-источник <code>bix</code>, которая хранит в БД <code>bixdb</code> схему <code>public</code>, в которой лежит таблица <code>claimitems</code>. Тогда скрипт будет называться <code>bix__bixdb__public__claimitems.sql</code>.<br>
Путь до ddl для таблиц: <code>cids\DB\Spark\CIDS\ICEBERG\table\&lt;table_ddl_name&gt;.sql</code>.<br>
Имя ddl для БД: <code>&lt;source_system&gt;__&lt;database_name&gt;.sql</code><br>
Путь до ddl для БД: <code>cids\DB\Spark\CIDS\ICEBERG\tablespace\&lt;db_ddl_name&gt;.sql</code>. Используется папка tablespace, т.к. папка database пока не поддерживается девопс-конвеером и не проходит nft-тесты.<br>
Для нашего примера DDL-скрипт для БД будет называться <code>bix__bixdb.sql</code></li>
</ul>
<p>2.2 Создание потоков загрузки и их интеграция с УМом</p>

