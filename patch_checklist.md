# Чеклист разработки патча

## Сбор требований и постановка задачи

### Создание целевых сущностей.

Запросить у аналитиков ЛМку. Выяснить:
1. Какие поля должны быть ключевыми? Есть ли в этих полях null на источнике? (такое бывает) Если есть, то какое дефолтное значение?
2. По каким полям следует делать партицирование? Обычно используется временнАя отчетная дата на источнике, значение которой для записей не обновляется/обновляется редко.
3. Как правильно маппить типы данных? Точно ли маппинг полей в ЛМке корректный? [Ссылка](https://docs.google.com/document/d/1Rg-wZCup3R7jOHMSE7WNUlD97-ULrnNCThRC2m8rgOc/edit) на док с маппингом основных типов из Postgres и MongoDB в Hive и Iceberg.
4. Нужно ли забирать с источника комментарии к полям и сущностям? Если да, то есть ли они в ЛМке?
5. Нужно ли создавать в целевой системе БД, или она считается уже созданной? (по умолчанию - нужно)
6. Какие у таблиц должны быть метаполя и как их правильно заполнять? Подробнее про это см. далее

### Создание загрузок

Описать тип загрузки в целевую систему. Выяснить тип загрузки - потоковая или пакетная.

Пакетная загрузка делается с помощью DAG'ов в Airflow, управляемых из УМа. DAG'и разрабатываются в соответствии с [шаблоном](https://confluence.moscow.alfaintra.net/pages/viewpage.action?pageId=1780104534) из документации ЕИСа.
Выяснить:
1. Какие из 6 тасок (типов загрузки) DAG'а следует реализовать?
2. В каком режиме будет работать DAG? Если по расписанию, то по какому?
3. Какие параметры из УМа следует передавать в таски? Что следует вынести в УМ, что оставить в самих тасках к ДАГу? Основная идея - параметризовать все с помощью УМа настолько, насколько это возможно.
4. Нужно ли реализовать логирование в УМ? Какие параметры следует логировать? Как это правильно сделать с т.з. внутренней организации УМа? Логирование в УМе работает только с регламентами, но не с атомарными потоками. Причем только с регламентами, поставленным на расписание. Поэтому если нужно логироваться, то в любом случае придется делать регламент в УМе и ставить его на расписание.
5. Какие метаполя следует добавить в целевые сущности для хранения информации, связанной с загрузкой данных? По умолчанию берем полный список из 12 метаполей из [стандарта](https://confluence.moscow.alfaintra.net/pages/viewpage.action?pageId=1757988205) архитектуры ЕИС. Чем заполнять эти поля при разных типах загрузки? Как эти поля будут обновляться с течением времени?

Потоковая загрузка: TBD.

## Разработка

Общие рекомендации:
1. Сразу именовать и раскладывать все по директориям в соответствии с [документацией](https://confluence.moscow.alfaintra.net/display/BDP/ICEBERG) ЕИСа. (!!!) Сэкономит много времени на этапе сдачи патча
2. В качестве референса следует использовать наш патч,  который одобрили и успешно выкатили на прод. Он лежит в ветке `CIDS-3` в рабочем [репозитории](https://git.moscow.alfaintra.net/projects/BI_CIDS/repos/cids/browse?at=refs%2Fheads%2FCIDS-3) на битбакете.

Общий пайплайн разработки:
Пишем ddl -> создаем нужные сущности в приемнике -> создаем даги и таски к ним -> загружаем их на сервер -> запускаем и убеждаемся, что данные льются

### Создание целевых сущностей.

1. Где брать информацию? - При написании ddl сверяться с [документацией](https://confluence.moscow.alfaintra.net/display/BDP/ICEBERG). Там, в частности, описано:
* Как правильно называть сущности и БД для них
* Как правильно располагать сущности в бакете s3
* Какие у таблиц должны быть свойства `tblproperties`
* Как настроить в таблицах ключи и партицирование
2. Как должен выглядеть DDL для таблиц? - Последовательность команд: `DROP TABLE IF EXISTS <table_name>` -> `CREATE TABLE IF NOT EXISTS <table_name>` -> `ALTER TABLE <table_name> SET IDENTIFIER FIELDS <identifier_field_1>, <identifier_field_2>;` -> `ALTER TABLE <table_name> WRITE DISTRIBUTED BY PARTITION LOCALLY ORDERED BY <partition_field_1> ASC NULLS LAST, <partition_field_2> ASC NULLS LAST;`
3. Как должен выглядеть DDL для БД? - `DROP DATABASE IF EXISTS <db_name>` -> `CREATE DATABASE IF NOT EXISTS <db_name>`
4. Какие у таблицы должны быть свойства? - Полный сбор статистик (`write.metadata.metrics.column.<column_name> = 'full'`) следует указывать для всех ключевых атрибутов и атрибутов, по которым выполняется партицирование таблицы. Другие свойства можно указывать без изменений:
```
'write.parquet.compression-codec' = 'zstd',
'write.parquet.compression-level' = '3',
'history.expire.max-snapshot-age-ms' = '86400000'
'format' = 'iceberg/parquet',
'format-version' = '2',
'write.delete.mode' = 'merge-on-read',
'write.merge.mode' = 'merge-on-read',
'write.update.mode' = 'merge-on-read',
'write.distribution-mode' = 'hash',
'write.metadata.delete-after-commit.enabled' = 'true',
'write.metadata.previous-versions-max' = '100',
```
5. Платформа для сборки патча не умеет работать с кодировками UTF. Это незаметно до тех пор, пока у тебя исходники не содержат не-ASCII символов. В противном же случае, - например, если у тебя в файлах есть комментарии к атрибутам или к коду на русском языке, - при работе с этими файлами следует всегда (!) явно (!) переключать в редакторе кода кодировку с UTF-8 на Windows-1251. Иначе можно легко по невнимательности сохранить файл в неправильной кодировке и испортить его, придется откатываться до последнего коммита в гите и начинать всю работу заново. **Внимательнее с кодировками!**
6. DDL-скрипты готовы. Как правильно их назвать и сложить в патч?
Имя ddl для таблицы: `<source_system>__<database_name>__<schema_name>__<table_name>.sql`
Здесь:
* `<source_system>` - наименование системы-источника
* `<database_name>` - имя БД в источнике
* `<schema_name>` - имя схемы в БД на источнике
* `<table_name>` - имя таблицы в БД на источнике
Пример: пусть у нас есть система-источник `bix`, которая хранит в БД `bixdb` схему `public`, в которой лежит таблица `claimitems`. Тогда скрипт будет называться `bix__bixdb__public__claimitems.sql`.
Путь до ddl для таблиц: `cids\DB\Spark\CIDS\ICEBERG\table\<table_ddl_name>.sql`.
Имя ddl для БД: `<source_system>__<database_name>.sql`
Путь до ddl для БД: `cids\DB\Spark\CIDS\ICEBERG\tablespace\<db_ddl_name>.sql`. Используется папка tablespace, т.к. папка database пока не поддерживается девопс-конвеером и не проходит nft-тесты.
Для нашего примера DDL-скрипт для БД будет называться `bix__bixdb.sql`

### Создание DAG'ов загрузки

При разработке следует опираться на имеющийся [шаблон](https://confluence.moscow.alfaintra.net/pages/viewpage.action?pageId=1780104534) DAG
За примерами см. [референс](https://git.moscow.alfaintra.net/projects/BI_CIDS/repos/cids/browse?at=refs%2Fheads%2FCIDS-3)

Данный шаблон требует следующих доработок:
1. Поменять все комменты на английский, иначе будут потом проблемы с кодировками при сборке патча (см. выше)
2. Выкинуть из шаблона функции `join_path_to_config()`, `generate_params_from_yaml()`, `calculate_required_resources()` и все места, где происходит обращение к ним. Они были нужны на том этапе разработки, когда у нас не было интеграции с УМом, и мы были вынуждены использовать `.yaml`-конфиг для параметризации работы DAG'ов. Сейчас это не актуально
3. Добавить в таску ветвления `task_branch` считывание параметра ветвления `$$P_BRANCH_PARAM` из УМа
4. Реализовать таску `log_paramprevvalue` с помощью `SparkSubmitOperator`, передать туда нужные параметры из УМа
5. Поменять jinja-параметр `{{ spark_conn_id }}` на коннетор до среды, в которой будет разворачиваться спарк-приложение. В нашем случае это был коннектор до кубера `spark_k8s`
6. Во всех тасках заменить считывание из `xcom` на считывание из УМа. `xcom` был нужен, когда у нас использовался `.yaml`-конфиг, сейчас это не актуально
7. Поменять jinja-параметр `{{ conn_id }}` на коннектор до источника данных. Коннектор настраивается в веб-интерфейсе Airflow, на верхней панели управления вкладка `Admin` -> `Connections`
8. Для режимов загрузки, которые не планируется реализовывать, таски следует изменить со `SparkSubmitOperator` на `EmptyOperator`

### Создание скриптов для DAG'ов загрузки

Общие правила:
1. Везде в настройках спарк-сессии проставлять московский часовой пояс
2. Учесть, что параметры из УМа в скрипты передаются в виде строк, независимо от фактического типа данных, который они содержат по смыслу -> некоторые из полей надо явно кастить в нужный тип. Например, `$$P_DMSJOB` кастится в `int`, `$$P_LAST_AS_OF_DAY` - в `timestamp` через `to_timestamp` внутри запроса, и т.д.
3. В настройках сессии следует указывать количество ресрсов для спарк-сессии. Мы явно проставили количество памяти на один экзекьютор `spark.executor.memory=5G` для таско загрузки данных. При попытке добавить возможности динамического выделения памяти `spark.dynamicAllocation.enable=true` DAG падал, ввиду отсутствие доступа к логам среды запуска нам так и не удалось утсановить причину ошибки

Скрипт для полной перезагрузки таблицы `load_init_full`
1. Имена полей следует считывать с целевой таблицы через `spark.sql('select * from <table_name>').schema.fieldNames()`, а не писать явно длинной строкой в скрипте
2. Ключевые поля, в которых есть `null`'ы, заполняются значениями по умолчанию
3. Перед загрузкой данных в целевой таблице делается `TRUNCATE`, чтобы полностью очистить и залить данные по новой
4. Для записи данных в целевую таблицу следует использовать метод `DataFrame.writeTo(<table_name>).overwritePartitions()`. Он умеет автоматически мэтчить поля по именам и кастить их в нужный тип
5. Чем заполнять метаполя при вставке новой строки? - см. референс.

Скрипт для загрузки дельты изменений `load_reg_key_merge`
1. См. замечания 1, 2, 4 и 5 для `load_init_full`
2. Какие поля обновлять при обновлении строки? Чем их заполнять? - см. референс.

Скрипт для логирования `LAST_AS_OF_DAY`
1. 

### Создание DAG'ов обслуживания

TBD

### Создание скриптов для DAG'ов обслуживания

TBD

### Дальнейшие пожелания по разработке

1. Перенести в УМ вообще все, что сейчас статично задается в DAG'ах или скриптах к ним. А именно:
* SQL-запросы к источнику
* Коннекторы подключения к источнику
* Ресурсы спарк-сессии для таски
* Возможно, что-то ещё...

2. 

